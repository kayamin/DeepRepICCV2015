{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import  matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = Variable(torch.FloatTensor(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__and__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__idiv__',\n",
       " '__ilshift__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__irshift__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lshift__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmul__',\n",
       " '__rshift__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_advanced_index_add',\n",
       " '_advanced_index_select',\n",
       " '_cdata',\n",
       " '_check_advanced_indexing',\n",
       " '_new_with_metadata_file',\n",
       " '_set_index',\n",
       " '_sparse_mask',\n",
       " '_torch',\n",
       " '_write_metadata',\n",
       " 'abs',\n",
       " 'abs_',\n",
       " 'acos',\n",
       " 'acos_',\n",
       " 'add',\n",
       " 'add_',\n",
       " 'addbmm',\n",
       " 'addbmm_',\n",
       " 'addcdiv',\n",
       " 'addcdiv_',\n",
       " 'addcmul',\n",
       " 'addcmul_',\n",
       " 'addmm',\n",
       " 'addmm_',\n",
       " 'addmv',\n",
       " 'addmv_',\n",
       " 'addr',\n",
       " 'addr_',\n",
       " 'apply_',\n",
       " 'asin',\n",
       " 'asin_',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atan2_',\n",
       " 'atan_',\n",
       " 'baddbmm',\n",
       " 'baddbmm_',\n",
       " 'bernoulli',\n",
       " 'bernoulli_',\n",
       " 'bmm',\n",
       " 'btrifact',\n",
       " 'btrisolve',\n",
       " 'byte',\n",
       " 'cauchy_',\n",
       " 'ceil',\n",
       " 'ceil_',\n",
       " 'char',\n",
       " 'chunk',\n",
       " 'clamp',\n",
       " 'clamp_',\n",
       " 'clone',\n",
       " 'contiguous',\n",
       " 'copy_',\n",
       " 'cos',\n",
       " 'cos_',\n",
       " 'cosh',\n",
       " 'cosh_',\n",
       " 'cpu',\n",
       " 'cross',\n",
       " 'cuda',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'data',\n",
       " 'data_ptr',\n",
       " 'diag',\n",
       " 'dim',\n",
       " 'dist',\n",
       " 'div',\n",
       " 'div_',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'eig',\n",
       " 'element_size',\n",
       " 'eq',\n",
       " 'eq_',\n",
       " 'equal',\n",
       " 'exp',\n",
       " 'exp_',\n",
       " 'expand',\n",
       " 'expand_as',\n",
       " 'exponential_',\n",
       " 'fill_',\n",
       " 'float',\n",
       " 'floor',\n",
       " 'floor_',\n",
       " 'fmod',\n",
       " 'fmod_',\n",
       " 'frac',\n",
       " 'frac_',\n",
       " 'gather',\n",
       " 'ge',\n",
       " 'ge_',\n",
       " 'gels',\n",
       " 'geometric_',\n",
       " 'geqrf',\n",
       " 'ger',\n",
       " 'gesv',\n",
       " 'gt',\n",
       " 'gt_',\n",
       " 'half',\n",
       " 'histc',\n",
       " 'index',\n",
       " 'index_add_',\n",
       " 'index_copy_',\n",
       " 'index_fill_',\n",
       " 'index_select',\n",
       " 'int',\n",
       " 'inverse',\n",
       " 'is_contiguous',\n",
       " 'is_cuda',\n",
       " 'is_pinned',\n",
       " 'is_same_size',\n",
       " 'is_set_to',\n",
       " 'is_shared',\n",
       " 'is_signed',\n",
       " 'is_sparse',\n",
       " 'kthvalue',\n",
       " 'le',\n",
       " 'le_',\n",
       " 'lerp',\n",
       " 'lerp_',\n",
       " 'lgamma',\n",
       " 'lgamma_',\n",
       " 'log',\n",
       " 'log1p',\n",
       " 'log1p_',\n",
       " 'log_',\n",
       " 'log_normal_',\n",
       " 'long',\n",
       " 'lt',\n",
       " 'lt_',\n",
       " 'map2_',\n",
       " 'map_',\n",
       " 'masked_copy_',\n",
       " 'masked_fill_',\n",
       " 'masked_scatter_',\n",
       " 'masked_select',\n",
       " 'matmul',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'min',\n",
       " 'mm',\n",
       " 'mode',\n",
       " 'mul',\n",
       " 'mul_',\n",
       " 'multinomial',\n",
       " 'mv',\n",
       " 'narrow',\n",
       " 'ndimension',\n",
       " 'ne',\n",
       " 'ne_',\n",
       " 'neg',\n",
       " 'neg_',\n",
       " 'nelement',\n",
       " 'new',\n",
       " 'nonzero',\n",
       " 'norm',\n",
       " 'normal_',\n",
       " 'numel',\n",
       " 'numpy',\n",
       " 'orgqr',\n",
       " 'ormqr',\n",
       " 'permute',\n",
       " 'pin_memory',\n",
       " 'potrf',\n",
       " 'potri',\n",
       " 'potrs',\n",
       " 'pow',\n",
       " 'pow_',\n",
       " 'prod',\n",
       " 'pstrf',\n",
       " 'qr',\n",
       " 'random_',\n",
       " 'reciprocal',\n",
       " 'reciprocal_',\n",
       " 'remainder',\n",
       " 'remainder_',\n",
       " 'renorm',\n",
       " 'renorm_',\n",
       " 'repeat',\n",
       " 'resize_',\n",
       " 'resize_as_',\n",
       " 'round',\n",
       " 'round_',\n",
       " 'rsqrt',\n",
       " 'rsqrt_',\n",
       " 'scatter_',\n",
       " 'scatter_add_',\n",
       " 'select',\n",
       " 'set_',\n",
       " 'shape',\n",
       " 'share_memory_',\n",
       " 'short',\n",
       " 'sigmoid',\n",
       " 'sigmoid_',\n",
       " 'sign',\n",
       " 'sign_',\n",
       " 'sin',\n",
       " 'sin_',\n",
       " 'sinh',\n",
       " 'sinh_',\n",
       " 'size',\n",
       " 'sort',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'sqrt_',\n",
       " 'squeeze',\n",
       " 'squeeze_',\n",
       " 'std',\n",
       " 'storage',\n",
       " 'storage_offset',\n",
       " 'storage_type',\n",
       " 'stride',\n",
       " 'sub',\n",
       " 'sub_',\n",
       " 'sum',\n",
       " 'svd',\n",
       " 'symeig',\n",
       " 't',\n",
       " 't_',\n",
       " 'tan',\n",
       " 'tan_',\n",
       " 'tanh',\n",
       " 'tanh_',\n",
       " 'tolist',\n",
       " 'topk',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'transpose_',\n",
       " 'tril',\n",
       " 'tril_',\n",
       " 'triu',\n",
       " 'triu_',\n",
       " 'trtrs',\n",
       " 'trunc',\n",
       " 'trunc_',\n",
       " 'type',\n",
       " 'type_as',\n",
       " 'unfold',\n",
       " 'uniform_',\n",
       " 'unsqueeze',\n",
       " 'unsqueeze_',\n",
       " 'var',\n",
       " 'view',\n",
       " 'view_as',\n",
       " 'zero_']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(b.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 9291.6963\n",
       " 6038.0786\n",
       " 9407.4697\n",
       "    ⋮     \n",
       " 8109.0034\n",
       " 1029.3820\n",
       " 9543.2783\n",
       "[torch.FloatTensor of size 10000]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.data.uniform_(0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  7.,  12.,  13.,   5.,  10.,  10.,   8.,  15.,   9.,  15.,  15.,\n",
       "          7.,   4.,   7.,   8.,   8.,   3.,  11.,   9.,   8.,  13.,   7.,\n",
       "          8.,  11.,  13.,   7.,   7.,   9.,  12.,  11.,  12.,   8.,  12.,\n",
       "          6.,   7.,  14.,   3.,  11.,   8.,   7.,  12.,   6.,   9.,  12.,\n",
       "          7.,  11.,   9.,   5.,   8.,  11.,   7.,  11.,  11.,  14.,  14.,\n",
       "         12.,  11.,   9.,   9.,   8.,  10.,  12.,  14.,  12.,   9.,   9.,\n",
       "          8.,   9.,  14.,  10.,  15.,   9.,  11.,   8.,  14.,  10.,  17.,\n",
       "         17.,   5.,   8.,  10.,   6.,   4.,  16.,  14.,   9.,  10.,   4.,\n",
       "          9.,  10.,   3.,   8.,  15.,  11.,  11.,   7.,   8.,   7.,   5.,\n",
       "          9.,   8.,   5.,   9.,   9.,   3.,   8.,   7.,  15.,   9.,   7.,\n",
       "          3.,  12.,  11.,  13.,   8.,  13.,  10.,   8.,   9.,   8.,   4.,\n",
       "          4.,  14.,  18.,  10.,   5.,   7.,  10.,  16.,   9.,  11.,  11.,\n",
       "          6.,   6.,  14.,   9.,  15.,  10.,  12.,  11.,   6.,   3.,   7.,\n",
       "          7.,   6.,  17.,  11.,   6.,   7.,  10.,  11.,  10.,  15.,  10.,\n",
       "          5.,  10.,   6.,   9.,  16.,   9.,   8.,   7.,   5.,  10.,  14.,\n",
       "          9.,  14.,  14.,   7.,   7.,  12.,  11.,  12.,  13.,   8.,  14.,\n",
       "          7.,  13.,   7.,   9.,   7.,   6.,  11.,  12.,   8.,   8.,  18.,\n",
       "         16.,  13.,   9.,   9.,  12.,  12.,   9.,   6.,   7.,   9.,  10.,\n",
       "          6.,   5.,  12.,  14.,   4.,  10.,   5.,  12.,  12.,   6.,  10.,\n",
       "         11.,  10.,   7.,  10.,   9.,   8.,  10.,   9.,  16.,  10.,  15.,\n",
       "         13.,  15.,   4.,   7.,   9.,  10.,  11.,   6.,   8.,   8.,  10.,\n",
       "         10.,  11.,   9.,  10.,   5.,   8.,  14.,  10.,  16.,  14.,   7.,\n",
       "         10.,  11.,  12.,   8.,   7.,  10.,   7.,   7.,   8.,  22.,   7.,\n",
       "          9.,  12.,  14.,   8.,  13.,  10.,  13.,   7.,  11.,  13.,  13.,\n",
       "          7.,  12.,  10.,   7.,  11.,  13.,  10.,   8.,  13.,   8.,  14.,\n",
       "         15.,  14.,   9.,   8.,  11.,  11.,  13.,   8.,  12.,  10.,   3.,\n",
       "          5.,   6.,  11.,  15.,  17.,  12.,  12.,   9.,   7.,  14.,   7.,\n",
       "          7.,   8.,   6.,  11.,   8.,  10.,  11.,  18.,   8.,  10.,  14.,\n",
       "         11.,   9.,  13.,   6.,   8.,  14.,  11.,   8.,   7.,  13.,  12.,\n",
       "         11.,  11.,   8.,   7.,   6.,   5.,  14.,   6.,   9.,   8.,  11.,\n",
       "          8.,  14.,  12.,  13.,   8.,   3.,  15.,  12.,   7.,  10.,  10.,\n",
       "         14.,   8.,  14.,  12.,  12.,  12.,   6.,   6.,   6.,   8.,   8.,\n",
       "          8.,  11.,   9.,  10.,   9.,   6.,  10.,  12.,   8.,  10.,   6.,\n",
       "         12.,  19.,   9.,  13.,   5.,  11.,   8.,  12.,  15.,   8.,  15.,\n",
       "         11.,  19.,  11.,   5.,  14.,  10.,  11.,   9.,  12.,   6.,  15.,\n",
       "          5.,  12.,  12.,  14.,   8.,   8.,   2.,  11.,  16.,  10.,   8.,\n",
       "          8.,  13.,   8.,   5.,  17.,  16.,   9.,   8.,  12.,   7.,  11.,\n",
       "         11.,   6.,   7.,  10.,   9.,  11.,  13.,  10.,  17.,   8.,   4.,\n",
       "          8.,   7.,   6.,  12.,  10.,   8.,  10.,   8.,  16.,   5.,   6.,\n",
       "          8.,   6.,   9.,   8.,   6.,  10.,  13.,  17.,  11.,   7.,   9.,\n",
       "          6.,  16.,  14.,   7.,  10.,   9.,  19.,  10.,  14.,   8.,   8.,\n",
       "         16.,   7.,  11.,   6.,  11.,  10.,   5.,   9.,  14.,   8.,  12.,\n",
       "         10.,   7.,   4.,  13.,  12.,   8.,  12.,   7.,   9.,  10.,  11.,\n",
       "         12.,   9.,  12.,   5.,   3.,  11.,  13.,  10.,  11.,  12.,  15.,\n",
       "          8.,  10.,   5.,  12.,  13.,  11.,  10.,  11.,  11.,   9.,   9.,\n",
       "          5.,  11.,  17.,   6.,  13.,  11.,   8.,  11.,   9.,  11.,  12.,\n",
       "         15.,  11.,  16.,  10.,  13.,  14.,  11.,   7.,   9.,   5.,  16.,\n",
       "          7.,   9.,   6.,   7.,  11.,  12.,   9.,  10.,  14.,   8.,  13.,\n",
       "          7.,  15.,  10.,  10.,  14.,   9.,   8.,   5.,  11.,   8.,   6.,\n",
       "         11.,   5.,  13.,  11.,  12.,  11.,   9.,  13.,   5.,   5.,   8.,\n",
       "          7.,  12.,   6.,  13.,   7.,   3.,   5.,  13.,  19.,   8.,  11.,\n",
       "         11.,   5.,  11.,  10.,   9.,   6.,  17.,   8.,  10.,  11.,   6.,\n",
       "          9.,  15.,   9.,   8.,  11.,  10.,  12.,   7.,  11.,  14.,  10.,\n",
       "         12.,  12.,  14.,  11.,   8.,   9.,  12.,   7.,  11.,   9.,  16.,\n",
       "          8.,  14.,   8.,   8.,   8.,   5.,   9.,  12.,   4.,  11.,  12.,\n",
       "          8.,  11.,   9.,  10.,  15.,  10.,   5.,  13.,   8.,   9.,   8.,\n",
       "         11.,  18.,  10.,  12.,  10.,  13.,   7.,  14.,   8.,  11.,  12.,\n",
       "         12.,   8.,   9.,   6.,   9.,  13.,   9.,   9.,  16.,  16.,  14.,\n",
       "         12.,   9.,  11.,  11.,   5.,   8.,  14.,   8.,  16.,   9.,  15.,\n",
       "          8.,   9.,  15.,   7.,  10.,  11.,  16.,   8.,  13.,  14.,  14.,\n",
       "          9.,   7.,   8.,  14.,   5.,   6.,  10.,   6.,  12.,  10.,  10.,\n",
       "          9.,  13.,   9.,  14.,   8.,  11.,   7.,  12.,   8.,   9.,  14.,\n",
       "         12.,  12.,  12.,  14.,   4.,   6.,  15.,  10.,   9.,  11.,   9.,\n",
       "         11.,  10.,  13.,   8.,   8.,  12.,   4.,  10.,  11.,  13.,  13.,\n",
       "         11.,   4.,   5.,  13.,  10.,   6.,  10.,   6.,  11.,   8.,  16.,\n",
       "          6.,  10.,   9.,  16.,  13.,   8.,  13.,   9.,   6.,   5.,  13.,\n",
       "         12.,   9.,   7.,  10.,  15.,  10.,  17.,   5.,   9.,  16.,   8.,\n",
       "          9.,  15.,   8.,  18.,  12.,  12.,  13.,  10.,  10.,   9.,  10.,\n",
       "          9.,  10.,   6.,  10.,  14.,   8.,  11.,  13.,  11.,   9.,  10.,\n",
       "          5.,  10.,  22.,  10.,  14.,  15.,   8.,   7.,   4.,  15.,  13.,\n",
       "          6.,   7.,  11.,   6.,   8.,  10.,  12.,  11.,  11.,   7.,   9.,\n",
       "         10.,   9.,  18.,   8.,   9.,  10.,   8.,  10.,   8.,   8.,   8.,\n",
       "          6.,  16.,  15.,  14.,   9.,  10.,   8.,   9.,  13.,  10.,  10.,\n",
       "          3.,   9.,  13.,  10.,  10.,   9.,   9.,  12.,   9.,  12.,  10.,\n",
       "         14.,  10.,   7.,  13.,  12.,   9.,   8.,   4.,   8.,   5.,  13.,\n",
       "          8.,  10.,   9.,   8.,  10.,   5.,  10.,   6.,   8.,   9.,  12.,\n",
       "          9.,  19.,  12.,  13.,   6.,   9.,  13.,   9.,   9.,  12.,   5.,\n",
       "         14.,  10.,  11.,  19.,  11.,  12.,  12.,  15.,   9.,  22.,   6.,\n",
       "          5.,   9.,  11.,   7.,  10.,   2.,  11.,   9.,  18.,   5.,   9.,\n",
       "         13.,   8.,   8.,   7.,  14.,  10.,   9.,  10.,  10.,   8.,  13.,\n",
       "          8.,   8.,   9.,   6.,  11.,  12.,  12.,  14.,   9.,   7.,   8.,\n",
       "          9.,  12.,   9.,  11.,  16.,   8.,   8.,   5.,   9.,  14.,  16.,\n",
       "         12.,   6.,   9.,   8.,  11.,  11.,  12.,  10.,  12.,   9.,  11.,\n",
       "          6.,   9.,   8.,   8.,  15.,  10.,  13.,  11.,  14.,  10.,   4.,\n",
       "         11.,  14.,  15.,  12.,   7.,   7.,  10.,   9.,  10.,  12.,   6.,\n",
       "          7.,  13.,   7.,  11.,  15.,  11.,   9.,   7.,  15.,   9.,  16.,\n",
       "          7.,   8.,   6.,   4.,  11.,  15.,  10.,   8.,  14.,   5.,  10.,\n",
       "          9.,  11.,   9.,   8.,   7.,  11.,  10.,  12.,  10.,  11.,  13.,\n",
       "         10.,  16.,  15.,  10.,  10.,  13.,   7.,  12.,   8.,   9.,  15.,\n",
       "          8.,  15.,  11.,  10.,  14.,  11.,  10.,   9.,  16.,   5.,  16.,\n",
       "          5.,  12.,  15.,   8.,  11.,   8.,   9.,  10.,   6.,   5.]),\n",
       " array([  1.51614666e-01,   1.01510627e+01,   2.01505107e+01, ...,\n",
       "          9.97960071e+03,   9.98960016e+03,   9.99959961e+03]),\n",
       " <a list of 1000 Patch objects>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADupJREFUeJzt3X+MZWddx/HPxy4tWAjddcfNWFpnaxqS/Ye2TJptMKZS\nKKUYiwkh3RhZpGaNAgEhMVv5Q/2vEEUlGmC1lY2BCkKhTVutdW3SkJjVWa3t9sey27Iru9l2pxIp\n+o8Uvvxxn2lvJzNzzz3nub++834lN3N+n+c5z93P3rn3fuc4IgQAmH0/MekGAADqINABIAkCHQCS\nINABIAkCHQCSINABIAkCHQCSINABIAkCHQCS2DLOk23fvj0WFhbGeUoAmHlHjhx5PiLmBm031kBf\nWFjQ0tLSOE8JADPP9qkm2/GWCwAkQaADQBIEOgAkQaADQBIEOgAkQaADQBIEOgAkQaADQBIEOgAk\nQaADQBIEOgAkQaADQBIEOgAkQaADQBIEOgAkQaADQBIEOgAkQaADQBIEOgAkQaADQBIEOgAkQaAD\nQBIEOgAkQaADQBIDA932JbYfsv2E7cdtf6Qs32b7QdvHy8+to28uAGA9TV6hvyjp4xGxS9JuSR+0\nvUvSfkmHIuJySYfKPABgQgYGekScjYh/L9Pfl/SkpIsl3STpYNnsoKR3j6qRAIDBhnoP3faCpCsl\nHZa0IyLOllXPStpRtWUAgKE0DnTbr5X0NUkfjYgX+tdFREiKdfbbZ3vJ9tLy8nKnxgIA1tco0G2/\nSr0w/2JE3FUWP2d7vqyfl3RurX0j4kBELEbE4tzcXI02AwDW0ORbLpZ0u6QnI+LTfavukbS3TO+V\ndHf95gEAmtrSYJu3SPo1SY/ZfqQs+z1Jt0n6iu1bJJ2S9N7RNBEA0MTAQI+Ib0ryOquvq9scAEBb\nVIoCQBIEOgAkQaADQBIEOgAkQaBjrBb23zfpJgBpEegAkASBDgBJEOgAkASBDgBJEOgAkASBDgBJ\nEOgAkASBDgBJEOgAGmlTFEYh2XgR6ACQBIEOAEkQ6ACQBIEOAEkQ6ACQBIEOAEkQ6ACQBIEOAEkQ\n6ACQBIGu0VSzzVqF3LS2dxztmta+A8Mi0AEgCQIdAJIg0AEgCQIdAJIg0AEgCQIdAJIg0AEgCQId\nAJIg0PuMssCk1rGzF8EM27/+7Wtfm2m71qNuz7T1dxSy95FAB4AkCHQASIJAB4AkCHQASIJAB4Ak\nBga67Ttsn7N9tG/ZH9g+Y/uR8rhxtM0EAAzS5BX6FyTdsMbyP4mIK8rj/rrNAgAMa2CgR8TDkr47\nhrYAADro8h76h2w/Wt6S2VqtRQCAVtoG+mcl/ZykKySdlfTH621oe5/tJdtLy8vLLU/3soX9981U\n1WX229tNU1s2m5r/FjY6R3Zd+zjKauVhtQr0iHguIn4YET+S9JeSrt5g2wMRsRgRi3Nzc23bCQAY\noFWg257vm/0VSUfX2xYAMB5bBm1g+05J10rabvu0pN+XdK3tKySFpJOSfnOEbQQANDAw0CNizxqL\nbx9BWwAAHVApCgBJEOgAkASBDgBJEOgAkMSmCfRpLPAZtP8wxx9lkckob+026UKMcRtV39c61sqy\nWS2gG6dRj8W4bJpAB4DsCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkCHQASGLmA30cd20Zl1EW\nP2W5RuMwbEHXsPvUOnctbfpb41jDmNbn77T9+5r5QAcA9BDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAE\ngQ4ASRDoAJAEgQ4AScxMoDepxFp9e6/19pmWqq611Lyt3bDXrMu5msx3OW/t7Ws8B9oeo+Y1b7rN\nuJ/ztapP+/8dj6Mqc5qzoYmZCXQAwMYIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIYqYD\nfdjCoRrFDrUKVqahgGGjPg4qTqnZ/mm6Fk0Ld2retm3YfYcp4Kq5b5PjTZONnq9Ni69G9ZwflZkO\ndADAywh0AEiCQAeAJAh0AEiCQAeAJAYGuu07bJ+zfbRv2TbbD9o+Xn5uHW0zAQCDNHmF/gVJN6xa\ntl/SoYi4XNKhMg8AmKCBgR4RD0v67qrFN0k6WKYPSnp35XYBAIbU9j30HRFxtkw/K2lHpfYAAFrq\n/KFoRISkWG+97X22l2wvLS8vdzrXOG4b1qQKcNS3Txtm/7XaW+vWb12qXkdRVbde1d4wlZtdKv+6\nbj/M82oct1mrdb42/15Wj1nTazWOas1ZqvZerW2gP2d7XpLKz3PrbRgRByJiMSIW5+bmWp4OADBI\n20C/R9LeMr1X0t11mgMAaKvJ1xbvlPQvkt5o+7TtWyTdJuntto9LeluZBwBM0JZBG0TEnnVWXVe5\nLQCADqgUBYAkCHQASIJAB4AkCHQASCJNoI/qS/7DFBo1vX1ZrXbV3n9UBUJtj1GruGSjMWxSXDOK\nYqRxFzW12a/JdJvj1zjueoVlaxUorVe01LQgrc1t+yZ167o0gQ4Amx2BDgBJEOgAkASBDgBJEOgA\nkASBDgBJEOgAkASBDgBJDPxri9OqVmHNRkVBte78M0x7um7TZNtx3k2o6TlH1d5axVJN27ew/z6d\nvO1dnY7fpJBl1HfBatKHYZ+PtY+53r6jfH4PmxfjvqsRr9ABIAkCHQCSINABIAkCHQCSINABIAkC\nHQCSINABIAkCHQCSINABIIlNEei1bqPW9DiDKtba3J5qXBV0tW6Z1sU4rknTY0/yloE1bjs3yXEb\nx7nH8RyYJZsi0AFgMyDQASAJAh0AkiDQASAJAh0AkiDQASAJAh0AkiDQASCJmb0F3XomVQxQ8zZp\n01BY0/actYpL2hY4tb0FXG3TWJRS6xaGg9bXKk6b5DWcxvFrglfoAJAEgQ4ASRDoAJAEgQ4ASRDo\nAJBEp2+52D4p6fuSfijpxYhYrNEoAMDwanxt8Rcj4vkKxwEAdMBbLgCQRNdAD0n/aPuI7X01GgQA\naMcR0X5n++KIOGP7pyU9KOnDEfHwqm32SdonSZdeeumbT5061epcs1q5hck5edu7XvG8WT0/Laa1\nXairSwWz7SNNPqPs9Ao9Is6Un+ckfV3S1WtscyAiFiNicW5ursvpAAAbaB3oti+0/bqVaUnXSzpa\nq2EAgOF0+ZbLDklft71ynC9FxD9UaRUAYGitAz0inpH0poptAQB0wNcWASAJAh0AkiDQASAJAh0A\nkiDQkdasFOvMSjsx/Qh0AEiCQAeAJAh0AEiCQAeAJAh0AEiCQAeAJAh0AEiCQAeAJAh0AEiCQMem\nQUUmsiPQASAJAh0AkiDQASAJAh0AkiDQASAJAh0AkiDQASAJAh0AkiDQASAJAh0AkiDQASAJAh0A\nkiDQASAJAh0AkiDQASAJAh0AkiDQASAJAh0AkiDQASAJAh0AkiDQASAJAh0AkiDQASCJToFu+wbb\nx2yfsL2/VqMAAMNrHei2z5P0F5LeKWmXpD22d9VqGABgOF1eoV8t6UREPBMR/y/pbyXdVKdZAIBh\ndQn0iyV9p2/+dFkGAJiALaM+ge19kvaV2f+1fazlobZLer5Oq2YGfd4c6PMm4E926vPPNtmoS6Cf\nkXRJ3/wbyrJXiIgDkg50OI8kyfZSRCx2Pc4soc+bA33eHMbR5y5vufybpMtt77R9vqSbJd1Tp1kA\ngGG1foUeES/a/pCkBySdJ+mOiHi8WssAAEPp9B56RNwv6f5KbRmk89s2M4g+bw70eXMYeZ8dEaM+\nBwBgDCj9B4AkZiLQs/yJAduX2H7I9hO2H7f9kbJ8m+0HbR8vP7eW5bb9mdLvR21f1XesvWX747b3\nTqpPTdk+z/Z/2L63zO+0fbj07cvlg3XZvqDMnyjrF/qOcWtZfsz2OybTk2ZsX2T7q7afsv2k7Wuy\nj7Pt3ynP66O277T96mzjbPsO2+dsH+1bVm1cbb/Z9mNln8/Y9lANjIipfqj3gevTki6TdL6k/5S0\na9LtatmXeUlXlenXSfqWen824VOS9pfl+yV9skzfKOnvJVnSbkmHy/Jtkp4pP7eW6a2T7t+Avn9M\n0pck3VvmvyLp5jL9OUm/VaZ/W9LnyvTNkr5cpneVsb9A0s7ynDhv0v3aoL8HJf1GmT5f0kWZx1m9\nosJvS3pN3/i+P9s4S/oFSVdJOtq3rNq4SvrXsq3Lvu8cqn2TvkANLuA1kh7om79V0q2Tblelvt0t\n6e2SjkmaL8vmJR0r05+XtKdv+2Nl/R5Jn+9b/ortpu2hXo3CIUlvlXRvebI+L2nL6jFW71tT15Tp\nLWU7rx73/u2m7SHp9SXcvGp52nHWy5Xj28q43SvpHRnHWdLCqkCvMq5l3VN9y1+xXZPHLLzlkvJP\nDJRfMa+UdFjSjog4W1Y9K2lHmV6v77N2Tf5U0u9K+lGZ/ylJ/xMRL5b5/va/1Ley/ntl+1nq805J\ny5L+urzN9Fe2L1TicY6IM5L+SNJ/STqr3rgdUe5xXlFrXC8u06uXNzYLgZ6O7ddK+pqkj0bEC/3r\novdfc5qvHtn+JUnnIuLIpNsyRlvU+7X8sxFxpaT/U+9X8ZckHOet6v1xvp2SfkbShZJumGijJmDS\n4zoLgd7oTwzMCtuvUi/MvxgRd5XFz9meL+vnJZ0ry9fr+yxdk7dI+mXbJ9X7i5xvlfRnki6yvVIH\n0d/+l/pW1r9e0n9rtvp8WtLpiDhc5r+qXsBnHue3Sfp2RCxHxA8k3aXe2Gce5xW1xvVMmV69vLFZ\nCPQ0f2KgfGJ9u6QnI+LTfavukbTySfde9d5bX1n+vvJp+W5J3yu/2j0g6XrbW8sro+vLsqkTEbdG\nxBsiYkG9sfvniPhVSQ9Jek/ZbHWfV67Fe8r2UZbfXL4dsVPS5ep9gDR1IuJZSd+x/cay6DpJTyjx\nOKv3Vstu2z9ZnucrfU47zn2qjGtZ94Lt3eUavq/vWM1M+gOGhh9C3KjeN0KelvSJSbenQz9+Xr1f\nxx6V9Eh53Kjee4eHJB2X9E+StpXtrd5NRJ6W9Jikxb5jfUDSifL49Un3rWH/r9XL33K5TL1/qCck\n/Z2kC8ryV5f5E2X9ZX37f6Jci2Ma8tP/CfT1CklLZay/od63GVKPs6Q/lPSUpKOS/ka9b6qkGmdJ\nd6r3GcEP1PtN7Jaa4yppsVy/pyX9uVZ9sD7oQaUoACQxC2+5AAAaINABIAkCHQCSINABIAkCHQCS\nINABIAkCHQCSINABIIkfA3F2RGN4e4syAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e0cfc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(b.data.numpy(), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.04319239,  1.03169596,  1.05311501, ...,  0.99532932,\n",
       "        0.98236912,  1.08803213], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import pdb\n",
    "\n",
    "# 畳み込み -> MaxPooling -> ReLU を行うクラス\n",
    "# Live Repetition Counting ネットワーク\n",
    "\n",
    "class RepetitionCountingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RepetitionCountingNet, self).__init__()\n",
    "\n",
    "        Layers = [\n",
    "\n",
    "        nn.Conv2d(20, 40, 5, 1, 1, bias=True),  # 50x50x20 -> 46x46x40\n",
    "        nn.MaxPool2d(2),                      # 46x46x40 -> 23x23x40\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(40, 60, 3, 1, 1, bias=True),  # 23x23x40 -> 21x21x60\n",
    "        nn.MaxPool2d(2),                      # 21x21x60 -> 10x10x60\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(60, 90, 3, 1, 1, bias=True),  # 10x10x60 -> 8x8x90\n",
    "        nn.MaxPool2d(2),                      # 8x8x90 -> 4x4x90\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4*4*90, 500, bias=True),\n",
    "        nn.Linear(500, 8, bias=True)\n",
    "\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*Layers)\n",
    "        \n",
    "    def forward(input):\n",
    "        return input\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = RepetitionCountingNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-30-3033d9f6d6c3>(1)<module>()\n",
      "-> for i, m in enumerate(test.modules()):\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-30-3033d9f6d6c3>(1)<module>()\n",
      "-> for i, m in enumerate(test.modules()):\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-30-3033d9f6d6c3>(1)<module>()\n",
      "-> for i, m in enumerate(test.modules()):\n",
      "(Pdb) m\n",
      "Conv2d(20, 40, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-30-3033d9f6d6c3>(1)<module>()\n",
      "-> for i, m in enumerate(test.modules()):\n",
      "(Pdb) m\n",
      "MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-30-3033d9f6d6c3>(1)<module>()\n",
      "-> for i, m in enumerate(test.modules()):\n",
      "(Pdb) mc\n",
      "*** NameError: name 'mc' is not defined\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-30-3033d9f6d6c3>(1)<module>()\n",
      "-> for i, m in enumerate(test.modules()):\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-30-3033d9f6d6c3>(1)<module>()\n",
      "-> for i, m in enumerate(test.modules()):\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-30-3033d9f6d6c3>(1)<module>()\n",
      "-> for i, m in enumerate(test.modules()):\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-30-3033d9f6d6c3>(1)<module>()\n",
      "-> for i, m in enumerate(test.modules()):\n",
      "(Pdb) m\n",
      "Conv2d(60, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-30-3033d9f6d6c3>(1)<module>()\n",
      "-> for i, m in enumerate(test.modules()):\n",
      "(Pdb) m\n",
      "MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-30-3033d9f6d6c3>(1)<module>()\n",
      "-> for i, m in enumerate(test.modules()):\n",
      "(Pdb) m\n",
      "ReLU ()\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-30-3033d9f6d6c3>(1)<module>()\n",
      "-> for i, m in enumerate(test.modules()):\n",
      "(Pdb) m\n",
      "Linear (1440 -> 500)\n",
      "(Pdb) dir(m)\n",
      "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_all_buffers', '_apply', '_backend', '_backward_hooks', '_buffers', '_forward_hooks', '_forward_pre_hooks', '_modules', '_parameters', 'add_module', 'apply', 'bias', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'float', 'forward', 'half', 'in_features', 'load_state_dict', 'modules', 'named_children', 'named_modules', 'named_parameters', 'out_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'reset_parameters', 'share_memory', 'state_dict', 'train', 'training', 'type', 'weight', 'zero_grad']\n",
      "(Pdb) m.weight\n",
      "Parameter containing:\n",
      "-2.6056e-02 -1.5173e-02  4.0855e-03  ...   2.0917e-02 -7.2316e-03 -9.3118e-03\n",
      " 1.9834e-02  3.5453e-03  2.2534e-02  ...  -1.2902e-02 -1.5300e-02 -2.0076e-02\n",
      "-1.4559e-02  1.0259e-02  1.9610e-02  ...  -1.3470e-03  1.7549e-02 -8.5706e-03\n",
      "                ...                   ⋱                   ...                \n",
      " 1.3462e-03  6.4993e-03  2.0915e-02  ...   2.0933e-02  1.6694e-02  1.2492e-02\n",
      "-2.4948e-02 -1.8584e-02  1.8462e-02  ...  -8.3044e-03 -1.3931e-03  2.5872e-02\n",
      "-1.7238e-03 -3.7567e-04 -2.2209e-02  ...  -1.8779e-02  1.1094e-02  6.5611e-03\n",
      "[torch.FloatTensor of size 500x1440]\n",
      "\n",
      "(Pdb) dir(m.weight.data)\n",
      "['__add__', '__and__', '__bool__', '__class__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__iand__', '__idiv__', '__ilshift__', '__imul__', '__init__', '__init_subclass__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__rshift__', '__rsub__', '__rtruediv__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '__xor__', '_advanced_index_add', '_advanced_index_select', '_cdata', '_check_advanced_indexing', '_new_with_metadata_file', '_set_index', '_sparse_mask', '_torch', '_write_metadata', 'abs', 'abs_', 'acos', 'acos_', 'add', 'add_', 'addbmm', 'addbmm_', 'addcdiv', 'addcdiv_', 'addcmul', 'addcmul_', 'addmm', 'addmm_', 'addmv', 'addmv_', 'addr', 'addr_', 'apply_', 'asin', 'asin_', 'atan', 'atan2', 'atan2_', 'atan_', 'baddbmm', 'baddbmm_', 'bernoulli', 'bernoulli_', 'bmm', 'btrifact', 'btrisolve', 'byte', 'cauchy_', 'ceil', 'ceil_', 'char', 'chunk', 'clamp', 'clamp_', 'clone', 'contiguous', 'copy_', 'cos', 'cos_', 'cosh', 'cosh_', 'cpu', 'cross', 'cuda', 'cumprod', 'cumsum', 'data', 'data_ptr', 'diag', 'dim', 'dist', 'div', 'div_', 'dot', 'double', 'eig', 'element_size', 'eq', 'eq_', 'equal', 'exp', 'exp_', 'expand', 'expand_as', 'exponential_', 'fill_', 'float', 'floor', 'floor_', 'fmod', 'fmod_', 'frac', 'frac_', 'gather', 'ge', 'ge_', 'gels', 'geometric_', 'geqrf', 'ger', 'gesv', 'gt', 'gt_', 'half', 'histc', 'index', 'index_add_', 'index_copy_', 'index_fill_', 'index_select', 'int', 'inverse', 'is_contiguous', 'is_cuda', 'is_pinned', 'is_same_size', 'is_set_to', 'is_shared', 'is_signed', 'is_sparse', 'kthvalue', 'le', 'le_', 'lerp', 'lerp_', 'lgamma', 'lgamma_', 'log', 'log1p', 'log1p_', 'log_', 'log_normal_', 'long', 'lt', 'lt_', 'map2_', 'map_', 'masked_copy_', 'masked_fill_', 'masked_scatter_', 'masked_select', 'matmul', 'max', 'mean', 'median', 'min', 'mm', 'mode', 'mul', 'mul_', 'multinomial', 'mv', 'narrow', 'ndimension', 'ne', 'ne_', 'neg', 'neg_', 'nelement', 'new', 'nonzero', 'norm', 'normal_', 'numel', 'numpy', 'orgqr', 'ormqr', 'permute', 'pin_memory', 'potrf', 'potri', 'potrs', 'pow', 'pow_', 'prod', 'pstrf', 'qr', 'random_', 'reciprocal', 'reciprocal_', 'remainder', 'remainder_', 'renorm', 'renorm_', 'repeat', 'resize_', 'resize_as_', 'round', 'round_', 'rsqrt', 'rsqrt_', 'scatter_', 'scatter_add_', 'select', 'set_', 'shape', 'share_memory_', 'short', 'sigmoid', 'sigmoid_', 'sign', 'sign_', 'sin', 'sin_', 'sinh', 'sinh_', 'size', 'sort', 'split', 'sqrt', 'sqrt_', 'squeeze', 'squeeze_', 'std', 'storage', 'storage_offset', 'storage_type', 'stride', 'sub', 'sub_', 'sum', 'svd', 'symeig', 't', 't_', 'tan', 'tan_', 'tanh', 'tanh_', 'tolist', 'topk', 'trace', 'transpose', 'transpose_', 'tril', 'tril_', 'triu', 'triu_', 'trtrs', 'trunc', 'trunc_', 'type', 'type_as', 'unfold', 'uniform_', 'unsqueeze', 'unsqueeze_', 'var', 'view', 'view_as', 'zero_']\n",
      "(Pdb) m.weight.data.zero_()\n",
      "\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "[torch.FloatTensor of size 500x1440]\n",
      "\n",
      "(Pdb) m.weight\n",
      "Parameter containing:\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "[torch.FloatTensor of size 500x1440]\n",
      "\n",
      "(Pdb) m.bias\n",
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      " -1.1787\n",
      "  2.3753\n",
      "  2.3326\n",
      "  2.5629\n",
      " -0.9381\n",
      " -0.1718\n",
      " -0.9263\n",
      " -2.4959\n",
      " -2.4434\n",
      " -1.6526\n",
      "  2.0429\n",
      "  1.8575\n",
      "  1.5788\n",
      "  1.1261\n",
      " -0.7845\n",
      " -1.3913\n",
      "  1.3358\n",
      "  1.1399\n",
      "  0.5304\n",
      " -2.3711\n",
      " -1.7266\n",
      " -1.7218\n",
      " -2.3388\n",
      "  1.4507\n",
      "  1.7867\n",
      "  1.3406\n",
      " -0.3606\n",
      " -0.2334\n",
      " -0.2340\n",
      "  1.9651\n",
      "  0.9061\n",
      " -2.3609\n",
      "  0.0223\n",
      " -2.3684\n",
      " -1.7532\n",
      "  2.5236\n",
      " -2.5963\n",
      "  2.1241\n",
      "  1.0827\n",
      " -1.9509\n",
      " -1.3884\n",
      "  1.7247\n",
      "  0.0648\n",
      "  2.4535\n",
      "  1.0127\n",
      "  0.5737\n",
      "  1.2001\n",
      "  0.4193\n",
      " -0.6293\n",
      " -1.5308\n",
      " -2.4346\n",
      "  2.1344\n",
      "  1.6548\n",
      "  0.5256\n",
      " -0.0541\n",
      "  1.0678\n",
      "  1.7668\n",
      " -2.3248\n",
      " -2.5081\n",
      "  1.3889\n",
      "  0.8029\n",
      "  1.1429\n",
      " -0.8470\n",
      " -0.0544\n",
      " -1.0904\n",
      " -0.9844\n",
      " -0.0116\n",
      "  1.1135\n",
      "  0.4109\n",
      "  1.9440\n",
      " -0.5390\n",
      " -1.4756\n",
      " -2.1110\n",
      " -0.5051\n",
      " -0.5019\n",
      "  0.4159\n",
      " -1.7805\n",
      "  2.4991\n",
      "  1.5031\n",
      " -1.0322\n",
      " -1.2406\n",
      "  1.1182\n",
      " -0.3647\n",
      "  1.2960\n",
      "  2.3117\n",
      "  1.6314\n",
      "  0.7767\n",
      " -1.9486\n",
      " -2.5485\n",
      " -1.5250\n",
      " -1.6796\n",
      " -1.1707\n",
      "  1.6817\n",
      "  0.8793\n",
      "  2.3809\n",
      "  0.5696\n",
      " -1.6149\n",
      " -0.8140\n",
      " -2.0070\n",
      "  1.5924\n",
      " -0.8552\n",
      " -0.5716\n",
      " -0.3469\n",
      "  0.3838\n",
      "  1.4148\n",
      "  0.5963\n",
      "  0.5874\n",
      " -2.4278\n",
      "  2.0937\n",
      " -1.4886\n",
      "  0.2746\n",
      " -0.1626\n",
      " -1.3087\n",
      " -1.5049\n",
      " -0.9559\n",
      "  2.0166\n",
      " -2.0074\n",
      " -0.7127\n",
      "  1.3243\n",
      " -0.0176\n",
      "  1.5474\n",
      " -2.4004\n",
      " -1.6119\n",
      " -2.4935\n",
      " -2.0778\n",
      "  0.5867\n",
      " -1.0035\n",
      "  0.9230\n",
      " -2.6313\n",
      "  2.5678\n",
      " -2.3278\n",
      "  2.0577\n",
      "  2.0901\n",
      " -1.6523\n",
      " -0.6822\n",
      "  2.1346\n",
      " -1.0898\n",
      " -2.6237\n",
      "  0.3275\n",
      "  0.0727\n",
      " -1.7615\n",
      " -0.7188\n",
      "  0.2620\n",
      " -2.3727\n",
      "  1.6191\n",
      "  1.1118\n",
      " -1.3776\n",
      " -0.2150\n",
      "  0.5645\n",
      "  1.8158\n",
      " -0.4826\n",
      "  1.7075\n",
      " -1.0752\n",
      " -2.2489\n",
      " -1.8519\n",
      " -1.9058\n",
      "  0.0139\n",
      "  0.9392\n",
      " -0.5899\n",
      " -0.8164\n",
      "  2.1055\n",
      "  0.7160\n",
      "  1.9395\n",
      " -2.4149\n",
      " -1.6893\n",
      " -2.4373\n",
      "  0.1584\n",
      " -2.2350\n",
      "  2.1046\n",
      "  1.3684\n",
      "  0.3626\n",
      " -1.7642\n",
      " -0.6597\n",
      "  0.5450\n",
      "  1.2696\n",
      "  0.1739\n",
      " -2.1965\n",
      "  1.5914\n",
      "  0.3890\n",
      "  1.4714\n",
      "  1.0201\n",
      "  0.7064\n",
      "  1.4843\n",
      "  1.2924\n",
      "  2.3992\n",
      "  0.4165\n",
      " -0.8666\n",
      " -1.8279\n",
      "  2.3099\n",
      " -2.0077\n",
      " -1.3752\n",
      " -0.9109\n",
      "  1.4608\n",
      " -2.1761\n",
      "  0.0214\n",
      " -0.5753\n",
      "  2.0880\n",
      " -1.9534\n",
      " -2.6194\n",
      " -1.3380\n",
      " -1.4600\n",
      "  1.9997\n",
      "  0.7323\n",
      " -2.0909\n",
      " -0.1751\n",
      " -0.0023\n",
      " -0.9377\n",
      " -1.3893\n",
      " -0.2102\n",
      "  2.3227\n",
      "  0.9210\n",
      "  0.8588\n",
      "  2.2187\n",
      "  0.3353\n",
      "  0.3302\n",
      "  0.8746\n",
      "  1.7778\n",
      "  2.5305\n",
      " -0.7270\n",
      "  0.5108\n",
      "  1.5030\n",
      "  0.7402\n",
      "  0.9432\n",
      " -0.1775\n",
      " -1.8092\n",
      "  2.5704\n",
      " -0.3931\n",
      "  1.5244\n",
      " -1.8077\n",
      " -1.0699\n",
      " -2.1638\n",
      "  1.2681\n",
      "  1.4400\n",
      "  2.0366\n",
      "  1.4796\n",
      " -1.6213\n",
      " -2.2341\n",
      " -0.6484\n",
      " -1.1858\n",
      "  1.8589\n",
      "  0.8768\n",
      " -0.9966\n",
      "  2.6252\n",
      "  2.0924\n",
      "  0.2528\n",
      " -1.6139\n",
      "  1.8575\n",
      "  1.5963\n",
      "  0.1650\n",
      " -0.1700\n",
      " -2.2088\n",
      " -0.9640\n",
      " -2.2064\n",
      " -2.3839\n",
      " -0.2381\n",
      "  0.2608\n",
      "  0.1310\n",
      "  1.3294\n",
      " -1.4125\n",
      "  1.1843\n",
      "  0.6425\n",
      " -0.3935\n",
      " -1.8341\n",
      " -1.0606\n",
      " -1.2055\n",
      " -0.2137\n",
      "  0.0816\n",
      " -0.9349\n",
      " -1.3514\n",
      "  0.6294\n",
      "  1.8792\n",
      "  1.4905\n",
      " -2.4497\n",
      " -1.5366\n",
      "  1.6836\n",
      " -0.0153\n",
      "  0.1207\n",
      " -0.7880\n",
      "  0.9637\n",
      " -0.3375\n",
      "  0.0397\n",
      " -0.7644\n",
      "  0.8920\n",
      " -0.8375\n",
      "  1.1152\n",
      "  0.4805\n",
      "  1.4251\n",
      " -1.8636\n",
      " -1.6048\n",
      " -0.4893\n",
      " -1.1496\n",
      "  0.6495\n",
      "  0.4544\n",
      "  1.0970\n",
      " -1.0832\n",
      " -2.0599\n",
      " -2.4055\n",
      "  2.0439\n",
      "  1.6198\n",
      "  2.1918\n",
      " -1.0539\n",
      " -1.6683\n",
      "  1.1580\n",
      " -1.6871\n",
      "  0.5343\n",
      "  0.8272\n",
      " -2.1556\n",
      " -2.0074\n",
      " -2.2743\n",
      " -0.7767\n",
      "  1.7788\n",
      " -1.4987\n",
      " -2.0350\n",
      "  0.6796\n",
      "  1.6940\n",
      " -2.2256\n",
      " -0.0188\n",
      "  0.5666\n",
      "  1.1381\n",
      " -0.9947\n",
      " -1.3388\n",
      " -2.6141\n",
      " -0.6782\n",
      " -0.6141\n",
      " -1.5143\n",
      "  0.1459\n",
      " -0.8501\n",
      " -2.2786\n",
      "  0.6360\n",
      "  1.7229\n",
      " -2.6299\n",
      "  0.6341\n",
      "  1.6183\n",
      " -2.5050\n",
      " -1.2326\n",
      " -0.5585\n",
      "  2.4159\n",
      "  1.9013\n",
      " -0.2542\n",
      "  0.6252\n",
      "  1.0032\n",
      "  2.4050\n",
      " -2.2338\n",
      " -2.0109\n",
      " -1.7859\n",
      " -2.6346\n",
      "  1.8008\n",
      "  1.2340\n",
      " -1.7178\n",
      "  1.6531\n",
      "  1.1796\n",
      "  0.2711\n",
      "  1.1000\n",
      " -2.6315\n",
      "  1.9046\n",
      " -1.7785\n",
      "  0.2893\n",
      "  0.8976\n",
      " -0.9857\n",
      " -2.2765\n",
      " -0.5993\n",
      " -0.0053\n",
      " -0.2352\n",
      "  1.5012\n",
      " -0.7119\n",
      " -0.3188\n",
      "  2.5655\n",
      " -1.8983\n",
      "  0.7167\n",
      " -0.0352\n",
      "  0.0787\n",
      " -0.3159\n",
      "  2.0899\n",
      " -1.7972\n",
      "  0.0631\n",
      " -1.8479\n",
      "  1.4132\n",
      "  0.1627\n",
      "  0.7380\n",
      "  0.9579\n",
      " -1.9079\n",
      "  1.9974\n",
      "  1.4030\n",
      " -0.8280\n",
      " -0.0511\n",
      " -2.6035\n",
      "  1.4577\n",
      "  0.7989\n",
      "  2.1561\n",
      "  2.3118\n",
      "  1.1813\n",
      " -0.7776\n",
      "  0.0601\n",
      " -2.6059\n",
      "  2.1945\n",
      " -1.0852\n",
      "  0.9010\n",
      " -0.9408\n",
      "  2.5440\n",
      "  2.0874\n",
      " -1.0987\n",
      "  0.5175\n",
      "  0.5378\n",
      " -0.2944\n",
      "  2.2373\n",
      "  1.8349\n",
      " -2.4899\n",
      " -2.4970\n",
      " -0.8304\n",
      " -0.0523\n",
      " -0.1062\n",
      "  2.0926\n",
      " -1.6801\n",
      " -2.3655\n",
      " -0.4971\n",
      " -2.2386\n",
      " -0.9059\n",
      "  1.9270\n",
      " -0.3495\n",
      " -2.2209\n",
      "  1.1836\n",
      "  0.1617\n",
      "  1.5133\n",
      "  1.1124\n",
      " -1.1726\n",
      " -0.2889\n",
      " -0.6810\n",
      "  2.3127\n",
      "  0.9705\n",
      "  1.7300\n",
      "  2.1509\n",
      " -1.0313\n",
      " -0.4230\n",
      " -2.2316\n",
      "  1.3326\n",
      "  1.6872\n",
      "  2.2949\n",
      " -2.3538\n",
      "  1.1680\n",
      "  0.2196\n",
      " -0.2958\n",
      " -1.8167\n",
      "  2.2423\n",
      " -0.3676\n",
      "  1.0967\n",
      "  2.5141\n",
      "  1.4034\n",
      "  0.2872\n",
      "  1.3688\n",
      " -0.8804\n",
      "  1.1576\n",
      " -1.6614\n",
      " -0.5653\n",
      "  0.5662\n",
      "  2.3261\n",
      "  1.8301\n",
      "  1.4549\n",
      "  2.1118\n",
      "  1.0715\n",
      " -0.2794\n",
      "  0.7293\n",
      " -1.5039\n",
      " -2.5457\n",
      "  1.5571\n",
      "  1.7421\n",
      " -1.7004\n",
      "  1.2266\n",
      " -2.0366\n",
      "  0.0551\n",
      " -2.1368\n",
      " -0.3088\n",
      "  1.8632\n",
      " -2.2847\n",
      " -1.9798\n",
      "  2.3287\n",
      "  1.1148\n",
      " -2.5412\n",
      "  2.3314\n",
      " -1.2703\n",
      " -1.0868\n",
      " -1.0703\n",
      "  0.8594\n",
      " -2.4473\n",
      "  1.4697\n",
      "  1.4937\n",
      " -0.6038\n",
      "  1.2926\n",
      " -1.5428\n",
      " -1.1567\n",
      " -1.2847\n",
      "  0.1389\n",
      " -1.2676\n",
      "  1.2828\n",
      " -1.5847\n",
      "  0.3053\n",
      " -2.1889\n",
      " -2.1260\n",
      "  0.6337\n",
      " -1.8595\n",
      " -1.3143\n",
      "[torch.FloatTensor of size 500]\n",
      "\n",
      "(Pdb) m.bias.data\n",
      "\n",
      "1.00000e-02 *\n",
      " -1.1787\n",
      "  2.3753\n",
      "  2.3326\n",
      "  2.5629\n",
      " -0.9381\n",
      " -0.1718\n",
      " -0.9263\n",
      " -2.4959\n",
      " -2.4434\n",
      " -1.6526\n",
      "  2.0429\n",
      "  1.8575\n",
      "  1.5788\n",
      "  1.1261\n",
      " -0.7845\n",
      " -1.3913\n",
      "  1.3358\n",
      "  1.1399\n",
      "  0.5304\n",
      " -2.3711\n",
      " -1.7266\n",
      " -1.7218\n",
      " -2.3388\n",
      "  1.4507\n",
      "  1.7867\n",
      "  1.3406\n",
      " -0.3606\n",
      " -0.2334\n",
      " -0.2340\n",
      "  1.9651\n",
      "  0.9061\n",
      " -2.3609\n",
      "  0.0223\n",
      " -2.3684\n",
      " -1.7532\n",
      "  2.5236\n",
      " -2.5963\n",
      "  2.1241\n",
      "  1.0827\n",
      " -1.9509\n",
      " -1.3884\n",
      "  1.7247\n",
      "  0.0648\n",
      "  2.4535\n",
      "  1.0127\n",
      "  0.5737\n",
      "  1.2001\n",
      "  0.4193\n",
      " -0.6293\n",
      " -1.5308\n",
      " -2.4346\n",
      "  2.1344\n",
      "  1.6548\n",
      "  0.5256\n",
      " -0.0541\n",
      "  1.0678\n",
      "  1.7668\n",
      " -2.3248\n",
      " -2.5081\n",
      "  1.3889\n",
      "  0.8029\n",
      "  1.1429\n",
      " -0.8470\n",
      " -0.0544\n",
      " -1.0904\n",
      " -0.9844\n",
      " -0.0116\n",
      "  1.1135\n",
      "  0.4109\n",
      "  1.9440\n",
      " -0.5390\n",
      " -1.4756\n",
      " -2.1110\n",
      " -0.5051\n",
      " -0.5019\n",
      "  0.4159\n",
      " -1.7805\n",
      "  2.4991\n",
      "  1.5031\n",
      " -1.0322\n",
      " -1.2406\n",
      "  1.1182\n",
      " -0.3647\n",
      "  1.2960\n",
      "  2.3117\n",
      "  1.6314\n",
      "  0.7767\n",
      " -1.9486\n",
      " -2.5485\n",
      " -1.5250\n",
      " -1.6796\n",
      " -1.1707\n",
      "  1.6817\n",
      "  0.8793\n",
      "  2.3809\n",
      "  0.5696\n",
      " -1.6149\n",
      " -0.8140\n",
      " -2.0070\n",
      "  1.5924\n",
      " -0.8552\n",
      " -0.5716\n",
      " -0.3469\n",
      "  0.3838\n",
      "  1.4148\n",
      "  0.5963\n",
      "  0.5874\n",
      " -2.4278\n",
      "  2.0937\n",
      " -1.4886\n",
      "  0.2746\n",
      " -0.1626\n",
      " -1.3087\n",
      " -1.5049\n",
      " -0.9559\n",
      "  2.0166\n",
      " -2.0074\n",
      " -0.7127\n",
      "  1.3243\n",
      " -0.0176\n",
      "  1.5474\n",
      " -2.4004\n",
      " -1.6119\n",
      " -2.4935\n",
      " -2.0778\n",
      "  0.5867\n",
      " -1.0035\n",
      "  0.9230\n",
      " -2.6313\n",
      "  2.5678\n",
      " -2.3278\n",
      "  2.0577\n",
      "  2.0901\n",
      " -1.6523\n",
      " -0.6822\n",
      "  2.1346\n",
      " -1.0898\n",
      " -2.6237\n",
      "  0.3275\n",
      "  0.0727\n",
      " -1.7615\n",
      " -0.7188\n",
      "  0.2620\n",
      " -2.3727\n",
      "  1.6191\n",
      "  1.1118\n",
      " -1.3776\n",
      " -0.2150\n",
      "  0.5645\n",
      "  1.8158\n",
      " -0.4826\n",
      "  1.7075\n",
      " -1.0752\n",
      " -2.2489\n",
      " -1.8519\n",
      " -1.9058\n",
      "  0.0139\n",
      "  0.9392\n",
      " -0.5899\n",
      " -0.8164\n",
      "  2.1055\n",
      "  0.7160\n",
      "  1.9395\n",
      " -2.4149\n",
      " -1.6893\n",
      " -2.4373\n",
      "  0.1584\n",
      " -2.2350\n",
      "  2.1046\n",
      "  1.3684\n",
      "  0.3626\n",
      " -1.7642\n",
      " -0.6597\n",
      "  0.5450\n",
      "  1.2696\n",
      "  0.1739\n",
      " -2.1965\n",
      "  1.5914\n",
      "  0.3890\n",
      "  1.4714\n",
      "  1.0201\n",
      "  0.7064\n",
      "  1.4843\n",
      "  1.2924\n",
      "  2.3992\n",
      "  0.4165\n",
      " -0.8666\n",
      " -1.8279\n",
      "  2.3099\n",
      " -2.0077\n",
      " -1.3752\n",
      " -0.9109\n",
      "  1.4608\n",
      " -2.1761\n",
      "  0.0214\n",
      " -0.5753\n",
      "  2.0880\n",
      " -1.9534\n",
      " -2.6194\n",
      " -1.3380\n",
      " -1.4600\n",
      "  1.9997\n",
      "  0.7323\n",
      " -2.0909\n",
      " -0.1751\n",
      " -0.0023\n",
      " -0.9377\n",
      " -1.3893\n",
      " -0.2102\n",
      "  2.3227\n",
      "  0.9210\n",
      "  0.8588\n",
      "  2.2187\n",
      "  0.3353\n",
      "  0.3302\n",
      "  0.8746\n",
      "  1.7778\n",
      "  2.5305\n",
      " -0.7270\n",
      "  0.5108\n",
      "  1.5030\n",
      "  0.7402\n",
      "  0.9432\n",
      " -0.1775\n",
      " -1.8092\n",
      "  2.5704\n",
      " -0.3931\n",
      "  1.5244\n",
      " -1.8077\n",
      " -1.0699\n",
      " -2.1638\n",
      "  1.2681\n",
      "  1.4400\n",
      "  2.0366\n",
      "  1.4796\n",
      " -1.6213\n",
      " -2.2341\n",
      " -0.6484\n",
      " -1.1858\n",
      "  1.8589\n",
      "  0.8768\n",
      " -0.9966\n",
      "  2.6252\n",
      "  2.0924\n",
      "  0.2528\n",
      " -1.6139\n",
      "  1.8575\n",
      "  1.5963\n",
      "  0.1650\n",
      " -0.1700\n",
      " -2.2088\n",
      " -0.9640\n",
      " -2.2064\n",
      " -2.3839\n",
      " -0.2381\n",
      "  0.2608\n",
      "  0.1310\n",
      "  1.3294\n",
      " -1.4125\n",
      "  1.1843\n",
      "  0.6425\n",
      " -0.3935\n",
      " -1.8341\n",
      " -1.0606\n",
      " -1.2055\n",
      " -0.2137\n",
      "  0.0816\n",
      " -0.9349\n",
      " -1.3514\n",
      "  0.6294\n",
      "  1.8792\n",
      "  1.4905\n",
      " -2.4497\n",
      " -1.5366\n",
      "  1.6836\n",
      " -0.0153\n",
      "  0.1207\n",
      " -0.7880\n",
      "  0.9637\n",
      " -0.3375\n",
      "  0.0397\n",
      " -0.7644\n",
      "  0.8920\n",
      " -0.8375\n",
      "  1.1152\n",
      "  0.4805\n",
      "  1.4251\n",
      " -1.8636\n",
      " -1.6048\n",
      " -0.4893\n",
      " -1.1496\n",
      "  0.6495\n",
      "  0.4544\n",
      "  1.0970\n",
      " -1.0832\n",
      " -2.0599\n",
      " -2.4055\n",
      "  2.0439\n",
      "  1.6198\n",
      "  2.1918\n",
      " -1.0539\n",
      " -1.6683\n",
      "  1.1580\n",
      " -1.6871\n",
      "  0.5343\n",
      "  0.8272\n",
      " -2.1556\n",
      " -2.0074\n",
      " -2.2743\n",
      " -0.7767\n",
      "  1.7788\n",
      " -1.4987\n",
      " -2.0350\n",
      "  0.6796\n",
      "  1.6940\n",
      " -2.2256\n",
      " -0.0188\n",
      "  0.5666\n",
      "  1.1381\n",
      " -0.9947\n",
      " -1.3388\n",
      " -2.6141\n",
      " -0.6782\n",
      " -0.6141\n",
      " -1.5143\n",
      "  0.1459\n",
      " -0.8501\n",
      " -2.2786\n",
      "  0.6360\n",
      "  1.7229\n",
      " -2.6299\n",
      "  0.6341\n",
      "  1.6183\n",
      " -2.5050\n",
      " -1.2326\n",
      " -0.5585\n",
      "  2.4159\n",
      "  1.9013\n",
      " -0.2542\n",
      "  0.6252\n",
      "  1.0032\n",
      "  2.4050\n",
      " -2.2338\n",
      " -2.0109\n",
      " -1.7859\n",
      " -2.6346\n",
      "  1.8008\n",
      "  1.2340\n",
      " -1.7178\n",
      "  1.6531\n",
      "  1.1796\n",
      "  0.2711\n",
      "  1.1000\n",
      " -2.6315\n",
      "  1.9046\n",
      " -1.7785\n",
      "  0.2893\n",
      "  0.8976\n",
      " -0.9857\n",
      " -2.2765\n",
      " -0.5993\n",
      " -0.0053\n",
      " -0.2352\n",
      "  1.5012\n",
      " -0.7119\n",
      " -0.3188\n",
      "  2.5655\n",
      " -1.8983\n",
      "  0.7167\n",
      " -0.0352\n",
      "  0.0787\n",
      " -0.3159\n",
      "  2.0899\n",
      " -1.7972\n",
      "  0.0631\n",
      " -1.8479\n",
      "  1.4132\n",
      "  0.1627\n",
      "  0.7380\n",
      "  0.9579\n",
      " -1.9079\n",
      "  1.9974\n",
      "  1.4030\n",
      " -0.8280\n",
      " -0.0511\n",
      " -2.6035\n",
      "  1.4577\n",
      "  0.7989\n",
      "  2.1561\n",
      "  2.3118\n",
      "  1.1813\n",
      " -0.7776\n",
      "  0.0601\n",
      " -2.6059\n",
      "  2.1945\n",
      " -1.0852\n",
      "  0.9010\n",
      " -0.9408\n",
      "  2.5440\n",
      "  2.0874\n",
      " -1.0987\n",
      "  0.5175\n",
      "  0.5378\n",
      " -0.2944\n",
      "  2.2373\n",
      "  1.8349\n",
      " -2.4899\n",
      " -2.4970\n",
      " -0.8304\n",
      " -0.0523\n",
      " -0.1062\n",
      "  2.0926\n",
      " -1.6801\n",
      " -2.3655\n",
      " -0.4971\n",
      " -2.2386\n",
      " -0.9059\n",
      "  1.9270\n",
      " -0.3495\n",
      " -2.2209\n",
      "  1.1836\n",
      "  0.1617\n",
      "  1.5133\n",
      "  1.1124\n",
      " -1.1726\n",
      " -0.2889\n",
      " -0.6810\n",
      "  2.3127\n",
      "  0.9705\n",
      "  1.7300\n",
      "  2.1509\n",
      " -1.0313\n",
      " -0.4230\n",
      " -2.2316\n",
      "  1.3326\n",
      "  1.6872\n",
      "  2.2949\n",
      " -2.3538\n",
      "  1.1680\n",
      "  0.2196\n",
      " -0.2958\n",
      " -1.8167\n",
      "  2.2423\n",
      " -0.3676\n",
      "  1.0967\n",
      "  2.5141\n",
      "  1.4034\n",
      "  0.2872\n",
      "  1.3688\n",
      " -0.8804\n",
      "  1.1576\n",
      " -1.6614\n",
      " -0.5653\n",
      "  0.5662\n",
      "  2.3261\n",
      "  1.8301\n",
      "  1.4549\n",
      "  2.1118\n",
      "  1.0715\n",
      " -0.2794\n",
      "  0.7293\n",
      " -1.5039\n",
      " -2.5457\n",
      "  1.5571\n",
      "  1.7421\n",
      " -1.7004\n",
      "  1.2266\n",
      " -2.0366\n",
      "  0.0551\n",
      " -2.1368\n",
      " -0.3088\n",
      "  1.8632\n",
      " -2.2847\n",
      " -1.9798\n",
      "  2.3287\n",
      "  1.1148\n",
      " -2.5412\n",
      "  2.3314\n",
      " -1.2703\n",
      " -1.0868\n",
      " -1.0703\n",
      "  0.8594\n",
      " -2.4473\n",
      "  1.4697\n",
      "  1.4937\n",
      " -0.6038\n",
      "  1.2926\n",
      " -1.5428\n",
      " -1.1567\n",
      " -1.2847\n",
      "  0.1389\n",
      " -1.2676\n",
      "  1.2828\n",
      " -1.5847\n",
      "  0.3053\n",
      " -2.1889\n",
      " -2.1260\n",
      "  0.6337\n",
      " -1.8595\n",
      " -1.3143\n",
      "[torch.FloatTensor of size 500]\n",
      "\n",
      "(Pdb) type(m.bias)\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "(Pdb) type(m.bias.data)\n",
      "<class 'torch.FloatTensor'>\n",
      "(Pdb) m.bias.data.zero_()\n",
      "\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 500]\n",
      "\n",
      "(Pdb) m.bias.data\n",
      "\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 500]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(test.modules()):\n",
    "    display(i)\n",
    "    pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
